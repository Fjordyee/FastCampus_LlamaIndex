{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS로 벤치마크 데이터셋 현실적으로 생성해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "from llama_index.core import Document\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from pprint import pprint\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "documents = SimpleDirectoryReader('papers').load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context, conditional\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "\n",
    "# Generator_llm. : Golden Dataset 생성 시 활용하는 모델\n",
    "generator_llm = OpenAI(model=\"gpt-4o-mini\")\n",
    "# Critic_llm : 각종 지표에 대한 계산 모델\n",
    "critic_llm = OpenAI(model=\"gpt-4o\")\n",
    "embeddings = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "distributions = {\n",
    "    simple:0.2,\n",
    "    reasoning:0.3,\n",
    "    multi_context:0.3,\n",
    "    conditional:0.2\n",
    "}\n",
    "\n",
    "generator = TestsetGenerator.from_llama_index(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings=embeddings\n",
    ")\n",
    "distributions = {\n",
    "    simple:0.1,\n",
    "    reasoning:0.4,\n",
    "    multi_context:0.4,\n",
    "    conditional:0.1\n",
    "}\n",
    "\n",
    "testset = generator.generate_with_llamaindex_docs(documents, 10, distributions, with_debugging_logs=True)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비정상 QA 페어 전처리\n",
    "import pandas as pd\n",
    "ds = testset.to_dataset()\n",
    "\n",
    "# 답변이 안되는 QA페어\n",
    "exclude_idx = [1,4,9]\n",
    "\n",
    "ds_ex = ds.select(\n",
    "    (\n",
    "        i for i in range(len(ds))\n",
    "        if i not in set(exclude_idx)\n",
    "    )\n",
    ")\n",
    "\n",
    "ds_dict = ds_ex.to_dict()\n",
    "ds_df = pd.DataFrame(ds_dict)\n",
    "ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 쿼리에 클렌징 필요 시 :ds_df.at[2, 'question']= \"직위 변경 신청 조건은?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 후 생성 질문 더 필요 시 생성 작업 반복\n",
    "testset = generator.generate_with_llamaindex_docs(documents, 10, distributions, with_debugging_logs=True)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 2회차 결과 전처리 및 다른 DF로 저장\n",
    "ds = testset.to_dataset()\n",
    "\n",
    "# 답변이 안되는 QA페어\n",
    "exclude_idx = [2,3,7,8]\n",
    "\n",
    "ds_ex = ds.select(\n",
    "    (\n",
    "        i for i in range(len(ds))\n",
    "        if i not in set(exclude_idx)\n",
    "    )\n",
    ")\n",
    "\n",
    "ds_dict = ds_ex.to_dict()\n",
    "ds_df2 = pd.DataFrame(ds_dict)\n",
    "ds_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전처리 후 생성 질문 더 필요 시 생성 작업 반복\n",
    "testset = generator.generate_with_llamaindex_docs(documents, 10, distributions, with_debugging_logs=True)\n",
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 3회차 결과 전처리 및 다른 DF로 저장\n",
    "ds = testset.to_dataset()\n",
    "\n",
    "# 답변이 안되는 QA페어\n",
    "exclude_idx = [2,5,7,9]\n",
    "\n",
    "ds_ex = ds.select(\n",
    "    (\n",
    "        i for i in range(len(ds))\n",
    "        if i not in set(exclude_idx)\n",
    "    )\n",
    ")\n",
    "\n",
    "ds_dict = ds_ex.to_dict()\n",
    "ds_df3 = pd.DataFrame(ds_dict)\n",
    "ds_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_df = pd.concat([ds_df,ds_df2,ds_df3]).reset_index().iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "golden_df\n",
    "#golden_df[['evolution_type','question','ground_truth']].to_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 생성된 벤치마크에 Custom Query Engine 결과 붙히고 평가 받아보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bench = pd.read_csv('benchmark_papers.csv').iloc[:,1:]\n",
    "bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\")\n",
    "\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "naive_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 벤치마크 세트에 naive RAG 답안 생성\n",
    "def process_query(row):\n",
    "    result = naive_engine.query(row['question'])\n",
    "\n",
    "    answer = result.response\n",
    "\n",
    "    context = [node.node.text for node in result.source_nodes]\n",
    "\n",
    "    return pd.Series({'answer': answer, 'contexts': context})\n",
    "\n",
    "bench[['answer', 'contexts']] = bench.apply(process_query, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import evaluate as evaluate_vanilla\n",
    "from datasets import Dataset \n",
    "eval_set= Dataset.from_dict(bench.to_dict(orient='list'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "#from langchain_openai.llms import AzureOpenAI, OpenAI\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    answer_similarity,\n",
    "    context_recall,\n",
    ")\n",
    "\n",
    "metrics = [\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness,\n",
    "    answer_similarity,\n",
    "    answer_relevancy\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_openai.llms import OpenAI\n",
    "result = evaluate_vanilla(embeddings=OpenAIEmbeddings(model='text-embedding-3-small'),\n",
    "    dataset=eval_set,\n",
    "    metrics=[\n",
    "        context_precision,\n",
    "        faithfulness,\n",
    "        answer_similarity,\n",
    "        context_recall\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_result(df,idx):\n",
    "    print(f\"질문: {df.loc[idx]['question']}\")\n",
    "    print('\\n')\n",
    "    print(f\"<Ground Truth>\\n{df.loc[idx]['ground_truth']}\\n\")\n",
    "    print(f\"<Naive RAG Answer>\\n{df.loc[idx]['answer']}\\n\")\n",
    "    print(f\"<Retrieved Contexts>\\n{df.loc[idx]['contexts']}\")\n",
    "    #print(f\"<Advanced RAG Answer>\\n{df.loc[idx]['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_result(result.to_pandas(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
