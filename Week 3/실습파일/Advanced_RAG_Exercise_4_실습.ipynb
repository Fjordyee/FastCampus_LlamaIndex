{"cells":[{"cell_type":"markdown","metadata":{"id":"QIw0W9N3ZWQD"},"source":["# Query Rewrite\n","- 질문 복잡성이 높을 때\n","- 앞단에 라우터 걸어서 복잡성 높은 쿼리에 대한 셀렉터 모듈로 활용될 수 있는 구조"]},{"cell_type":"code","source":["!pip install llama_index openai datasets llama-index-retrievers-bm25"],"metadata":{"id":"mcN4vCTzZrdo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# set up OpenAI\n","import os\n","import getpass\n","\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n","import openai\n","\n","openai.api_key = os.environ[\"OPENAI_API_KEY\"]"],"metadata":{"id":"DbFWefz0Z6e_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySojN65-ZWQJ"},"outputs":[],"source":["\n","\n","import logging\n","import sys\n","\n","\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n","from llama_index.core.indices.query.query_transform import HyDEQueryTransform\n","from llama_index.core.query_engine import TransformQueryEngine\n","from IPython.display import Markdown, display\n","import pprint\n","from llama_index.llms.openai import OpenAI\n","from llama_index.embeddings.openai import OpenAIEmbedding\n","from llama_index.core import Settings\n","\n","#글로벌 세팅\n","Settings.embed_model = OpenAIEmbedding(\n","    model=\"text-embedding-3-small\"\n",")\n","Settings.llm= OpenAI(model='gpt-4o-mini')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IcuBev_CZWQM"},"outputs":[],"source":["# Dataset 로드\n","from datasets import load_dataset\n","\n","ds = load_dataset(\"HAERAE-HUB/KOREAN-WEBTEXT\", split='train[:20]')\n","data = ds.to_pandas()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpg2iOBRZWQN"},"outputs":[],"source":["data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XsgQFXekZWQN"},"outputs":[],"source":["# Document 오브젝트로 변환\n","from llama_index.core import Document, VectorStoreIndex\n","docs = []\n","\n","#Iterative하게 Document 만들기\n","for i, row in data.iterrows():\n","    docs.append(Document(\n","        text=row['text'],\n","        # extra_info={'title': row['title']}\n","    ))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-TP7RpEiZWQO"},"outputs":[],"source":["index = VectorStoreIndex.from_documents(\n","    docs\n",")"]},{"cell_type":"markdown","metadata":{"id":"Z5jqZigUZWQO"},"source":["## 1. 커스텀 프롬프트 활용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"koWx3ZLOZWQP"},"outputs":[],"source":["from llama_index.core import PromptTemplate\n","from llama_index.llms.openai import OpenAI\n","\n","\n","# 쿼리 분해 프롬프트 작성\n","query_gen_str = \"\"\"\n","너는 사용자가 대충 쓴 질문에 대해서, 최대한 답변하기 위한 근거를 찾기 위한 다수의 서치 쿼리를 생성해 내야해.\n","생성하는 쿼리 중에 딱 하나는 역으로 된 질문으로 추가해서, 상대적 근거로 활용될 수 있도록 해야해.\n","{num_queries}개의 서치 쿼리를 만들어 내고, 하나당 한줄씩 사용해.\n","Query: {query}\n","Queries:\n","\"\"\"\n","\n","# 프롬프트템플릿화\n","query_gen_prompt = PromptTemplate(query_gen_str)\n","\n","#쿼리 브레이커에 사용할 llm 정의 - 최고의 성능모델이 아니어도 됨\n","llm =\n","\n","# 커스텀 제너레이터\n","def generate_queries(llm, query: str, num_queries: int = 4):\n","    response = llm.predict(\n","        query_gen_prompt, num_queries=num_queries, query=query\n","    )\n","    queries = response.split(\"\\n\")\n","    queries_str = \"\\n\".join(queries)\n","    print(f\"Generated queries:\\n{queries_str}\")\n","    return queries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O5RUsVRXZWQP"},"outputs":[],"source":["# 잘 답변못했던 질문 확인\n","queries ="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hBTS_bVXZWQQ"},"outputs":[],"source":["queries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiV7l_PVZWQQ"},"outputs":[],"source":["from tqdm.asyncio import tqdm\n","import nest_asyncio\n","nest_asyncio.apply()\n","\n","# 다수의 쿼리를 동시에 날려야 하기 떄문에 async 사용\n","# 사용할 다수 Retriever들의 결과를 합치는 용도\n","async def run_queries(queries, retrievers):\n","    tasks = []\n","    for query in queries:\n","        for i, retriever in enumerate(retrievers):\n","            tasks.append()\n","\n","    task_results = await tqdm.gather(*tasks)\n","\n","    results_dict = {}\n","    for i, (query, query_result) in enumerate(zip(queries, task_results)):\n","        results_dict[(query, i)] = query_result\n","\n","    return results_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKET5VULZWQR"},"outputs":[],"source":["\n","from llama_index.retrievers.bm25 import BM25Retriever\n","\n","\n","## vector retriever - top 2\n","vector_retriever =\n","\n","## bm25 retriever - top 2\n","bm25_retriever = BM25Retriever.from_defaults(\n","    docstore=index.docstore, similarity_top_k=2\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZM-4X9wnZWQR"},"outputs":[],"source":["# 쿼리 실행\n","results_dict = await run_queries(queries, [vector_retriever, bm25_retriever])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiDQ_fgXZWQS"},"outputs":[],"source":["results_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YdHPcDeQZWQS"},"outputs":[],"source":["from typing import List\n","from llama_index.core.schema import NodeWithScore\n","\n","# 여러 Retriever 메서드 사용했기 때문에 결과 퓨전 필요\n","# 기본적인 rrf 사용\n","def fuse_results(results_dict, similarity_top_k: int = 2):\n","    \"\"\"Fuse results.\"\"\"\n","    k = 60.0\n","    fused_scores = {}\n","    text_to_node = {}\n","\n","    for nodes_with_scores in results_dict.values():\n","        for rank, node_with_score in enumerate(\n","            sorted(\n","                nodes_with_scores, key=lambda x: x.score or 0.0, reverse=True\n","            )\n","        ):\n","            text = node_with_score.node.get_content()\n","            text_to_node[text] = node_with_score\n","            if text not in fused_scores:\n","                fused_scores[text] = 0.0\n","            fused_scores[text] += 1.0 / (rank + k)\n","\n","    # fusion 스코어 기반 소팅\n","    reranked_results = dict(\n","        sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n","    )\n","    reranked_nodes: List[NodeWithScore] = []\n","    for text, score in reranked_results.items():\n","        reranked_nodes.append(text_to_node[text])\n","        reranked_nodes[-1].score = score\n","\n","    return reranked_nodes[:similarity_top_k]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bU6pmp5QZWQT"},"outputs":[],"source":["# 결과 퓨징\n","final_results ="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lJEb16wBZWQT"},"outputs":[],"source":["for n in final_results:\n","    print(n.score, \"\\n\", n.text, \"\\n********\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nufNyzU9ZWQT"},"outputs":[],"source":["from typing import List\n","\n","from llama_index.core import QueryBundle\n","from llama_index.core.retrievers import BaseRetriever\n","from llama_index.core.schema import NodeWithScore\n","import asyncio\n","\n","# 퓨전리트리버 클래스 정의\n","class FusionRetriever(BaseRetriever):\n","\n","    def __init__(\n","        self,\n","        llm,\n","        retrievers: List[BaseRetriever],\n","        similarity_top_k: int = 2,\n","    ) -> None:\n","        \"\"\"Init params.\"\"\"\n","        self._retrievers = retrievers\n","        self._similarity_top_k = similarity_top_k\n","        self._llm = llm\n","        super().__init__()\n","\n","    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n","        \"\"\"Retrieve.\"\"\"\n","        queries = generate_queries(\n","            self._llm, query_bundle.query_str, num_queries=4\n","        )\n","        results = asyncio.run(run_queries(queries, self._retrievers))\n","        final_results = fuse_results(\n","            results, similarity_top_k=self._similarity_top_k\n","        )\n","\n","        return final_results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWE9WKYUZWQU"},"outputs":[],"source":["from llama_index.core.query_engine import RetrieverQueryEngine\n","\n","#퓨전리트리버 생성\n","fusion_retriever = FusionRetriever(\n","\n",")\n","# 퓨전리트리버를 쿼리엔진으로 사용\n","query_engine ="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rTQFGhRvZWQU"},"outputs":[],"source":["query_str="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VGpN4RW0ZWQV"},"outputs":[],"source":["#naive\n","naive_query_engine =\n","response =\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"on--eqWqZWQV"},"outputs":[],"source":["print(str(response))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"484qwyeqZWQV"},"outputs":[],"source":["# 퓨전리트리버 답안\n","response2 = query_engine.query(query_str)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GG4mkCXZWQW"},"outputs":[],"source":["print(str(response2))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0pV2yI9bZWQW"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[{"file_id":"1jVBiUv-CZZJZDVHoXwLIt03bus9VuPia","timestamp":1722525263336}]}},"nbformat":4,"nbformat_minor":0}